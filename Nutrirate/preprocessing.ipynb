{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a5ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment Ready. Paths set.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We use r\"...\" to handle Windows backslashes correctly\n",
    "BASE_DIR = r\"C:\\Users\\arjun\\Downloads\\Nutri Rate\"\n",
    "PARQUET_FILE = r\"C:\\Users\\arjun\\Downloads\\food.parquet\"\n",
    "INDIAN_CSV = r\"C:\\Users\\arjun\\Downloads\\Indian_Food_Nutrition_Processed.csv\"\n",
    "USDA_FOLDER = r\"C:\\Users\\arjun\\Downloads\\FoodData_Central_csv_2025-12-18\\FoodData_Central_csv_2025-12-18\"\n",
    "\n",
    "print(\"‚úÖ Environment Ready. Paths set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1c830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Grading Logic Defined.\n"
     ]
    }
   ],
   "source": [
    "def calculate_nutri_grade(row):\n",
    "    \"\"\"\n",
    "    Calculates a simplified Nutri-Score (A-E) based on:\n",
    "    - Points for Bad stuff (Sugar, Sat Fat, Calories)\n",
    "    - Points for Good stuff (Protein, Fiber)\n",
    "    \"\"\"\n",
    "    # 1. if Grade already exists, keep it\n",
    "    if pd.notna(row.get('grade')) and row['grade'] in ['A', 'B', 'C', 'D', 'E']:\n",
    "        return row['grade']\n",
    "\n",
    "    # 2. If missing crucial data, return Unknown\n",
    "    if pd.isna(row.get('sugar')) or pd.isna(row.get('fat')):\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    # 3. Calculate \"Bad Points\" (N)\n",
    "    points_n = 0\n",
    "    # Energy\n",
    "    if row['calories'] > 335: points_n += 4\n",
    "    elif row['calories'] > 100: points_n += 2\n",
    "    # Sugar\n",
    "    if row['sugar'] > 45: points_n += 10\n",
    "    elif row['sugar'] > 22: points_n += 6\n",
    "    elif row['sugar'] > 4.5: points_n += 2\n",
    "    # Sat Fat (Approximation)\n",
    "    if row['fat'] > 10: points_n += 10\n",
    "    elif row['fat'] > 4: points_n += 4\n",
    "\n",
    "    # 4. Calculate \"Good Points\" (P)\n",
    "    points_p = 0\n",
    "    # Protein\n",
    "    if row['protein'] > 8: points_p += 5\n",
    "    elif row['protein'] > 1.6: points_p += 2\n",
    "\n",
    "    # 5. Final Score & Mapping\n",
    "    final_score = points_n - points_p\n",
    "\n",
    "    if final_score <= -1: return 'A'    # Very Healthy\n",
    "    if final_score <= 2: return 'B'     # Healthy\n",
    "    if final_score <= 10: return 'C'    # Balanced\n",
    "    if final_score <= 18: return 'D'    # Limit\n",
    "    return 'E'                          # Unhealthy\n",
    "\n",
    "print(\"‚úÖ Grading Logic Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d72726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns found: 144\n",
      "\n",
      "--- CHECKING REQUIRED COLUMNS ---\n",
      "‚úÖ Found: code\n",
      "‚ùå MISSING: product_name\n",
      "‚ùå MISSING: countries_tags\n",
      "‚úÖ Found: nutriscore_grade\n",
      "‚ùå MISSING: energy-kcal_100g\n",
      "‚ùå MISSING: energy_100g\n",
      "‚ùå MISSING: sugars_100g\n",
      "‚ùå MISSING: fat_100g\n",
      "‚ùå MISSING: proteins_100g\n",
      "‚ùå MISSING: image_url\n",
      "‚ùå MISSING: image_small_url\n",
      "\n",
      "--- FIRST 20 COLUMNS (For Reference) ---\n",
      "['additives_n', 'element', 'element', 'element', 'brands', 'categories', 'element', 'ciqual_food_code', 'agribalyse_food_code', 'agribalyse_proxy_food_code', 'element', 'element', 'element', 'code', 'compared_to_category', 'complete', 'completeness', 'element', 'element', 'created_t']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Load the file metadata (this is instant)\n",
    "parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "\n",
    "# Get all column names\n",
    "all_columns = parquet_file.schema.names\n",
    "\n",
    "print(f\"Total Columns found: {len(all_columns)}\")\n",
    "\n",
    "# Check specifically for the ones we need\n",
    "required_checks = [\n",
    "    'code', 'product_name', 'countries_tags', 'nutriscore_grade',\n",
    "    'energy-kcal_100g', 'energy_100g', \n",
    "    'sugars_100g', 'fat_100g', 'proteins_100g', \n",
    "    'image_url', 'image_small_url'\n",
    "]\n",
    "\n",
    "print(\"\\n--- CHECKING REQUIRED COLUMNS ---\")\n",
    "found_cols = []\n",
    "for col in required_checks:\n",
    "    if col in all_columns:\n",
    "        print(f\"‚úÖ Found: {col}\")\n",
    "        found_cols.append(col)\n",
    "    else:\n",
    "        print(f\"‚ùå MISSING: {col}\")\n",
    "\n",
    "# Print first 20 columns to see the naming style\n",
    "print(\"\\n--- FIRST 20 COLUMNS (For Reference) ---\")\n",
    "print(all_columns[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3086048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns: 144\n",
      "\n",
      "--- üîç COLUMN SEARCH RESULTS ---\n",
      "\n",
      "üëâ Searching for 'NAME':\n",
      "   Found: name\n",
      "   Found: field_name\n",
      "\n",
      "üëâ Searching for 'ENERGY':\n",
      "   ‚ùå No match found.\n",
      "\n",
      "üëâ Searching for 'SUGAR':\n",
      "   ‚ùå No match found.\n",
      "\n",
      "üëâ Searching for 'FAT':\n",
      "   ‚ùå No match found.\n",
      "\n",
      "üëâ Searching for 'PROTEIN':\n",
      "   ‚ùå No match found.\n",
      "\n",
      "üëâ Searching for 'IMAGE':\n",
      "   Found: last_image_t\n",
      "\n",
      "üëâ Searching for 'URL':\n",
      "   ‚ùå No match found.\n",
      "\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Load the file metadata again\n",
    "parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "all_columns = parquet_file.schema.names\n",
    "\n",
    "print(f\"Total Columns: {len(all_columns)}\")\n",
    "\n",
    "# Define keywords to search for\n",
    "search_terms = ['name', 'energy', 'sugar', 'fat', 'protein', 'image', 'url']\n",
    "\n",
    "print(\"\\n--- üîç COLUMN SEARCH RESULTS ---\")\n",
    "\n",
    "for term in search_terms:\n",
    "    print(f\"\\nüëâ Searching for '{term.upper()}':\")\n",
    "    matches = [c for c in all_columns if term in c.lower()]\n",
    "    \n",
    "    # Print the first 5 matches to avoid spamming\n",
    "    if matches:\n",
    "        for match in matches[:5]:\n",
    "            print(f\"   Found: {match}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå No match found.\")\n",
    "\n",
    "print(\"\\n---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0c03db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ALL 144 COLUMNS ---\n",
      "['additives_n', 'element', 'element', 'element', 'brands', 'categories', 'element', 'ciqual_food_code', 'agribalyse_food_code', 'agribalyse_proxy_food_code']\n",
      "['element', 'element', 'element', 'code', 'compared_to_category', 'complete', 'completeness', 'element', 'element', 'created_t']\n",
      "['creator', 'element', 'element', 'element', 'element', 'ecoscore_data', 'ecoscore_grade', 'ecoscore_score', 'element', 'element']\n",
      "['element', 'emb_codes', 'element', 'element', 'lang', 'text', 'key', 'imgid', 'rev', 'h']\n",
      "['w', 'h', 'w', 'h', 'w', 'h', 'w', 'uploaded_t', 'uploader', 'element']\n",
      "['element', 'ingredients_from_palm_oil_n', 'ingredients_n', 'element', 'ingredients_percent_analysis', 'element', 'lang', 'text', 'ingredients_with_specified_percent_n', 'ingredients_with_unspecified_percent_n']\n",
      "['ingredients_without_ciqual_codes_n', 'element', 'ingredients', 'known_ingredients_n', 'element', 'labels', 'lang', 'element', 'element', 'last_editor']\n",
      "['last_image_t', 'last_modified_by', 'last_modified_t', 'last_updated_t', 'link', 'element', 'element', 'manufacturing_places', 'max_imgid', 'element']\n",
      "['element', 'new_additives_n', 'no_nutrition_data', 'nova_group', 'element', 'nova_groups', 'element', 'element', 'name', 'value']\n",
      "['100g', 'serving', 'unit', 'prepared_value', 'prepared_100g', 'prepared_serving', 'prepared_unit', 'nutriscore_grade', 'nutriscore_score', 'nutrition_data_per']\n",
      "['obsolete', 'element', 'origins', 'field_name', 'timestamp', 'owner', 'packagings_complete', 'element', 'element', 'element']\n",
      "['lang', 'text', 'packaging', 'material', 'number_of_units', 'quantity_per_unit', 'quantity_per_unit_unit', 'quantity_per_unit_value', 'recycling', 'shape']\n",
      "['weight_measured', 'element', 'popularity_key', 'element', 'lang', 'text', 'product_quantity_unit', 'product_quantity', 'element', 'quantity']\n",
      "['rev', 'scans_n', 'serving_quantity', 'serving_size', 'element', 'element', 'stores', 'element', 'unique_scans_n', 'unknown_ingredients_n']\n",
      "['element', 'element', 'with_non_nutritive_sweeteners', 'with_sweeteners']\n",
      "\n",
      "--- üîç INSPECTING FIRST ROW DATA ---\n",
      "\n",
      "üëâ Column 'categories_properties' contains: {'ciqual_food_code': 31032.0, 'agribalyse_food_code': 31032.0, 'agribalyse_proxy_food_code': 31032.0}\n",
      "\n",
      "‚úÖ FOUND 'nutriments' column! Content:\n",
      "[{'name': 'saturated-fat', 'value': 10.0, '100g': 10.0, 'serving': None, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'fruits-vegetables-nuts-estimate', 'value': 40.0, '100g': 40.0, 'serving': 40.0, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'sodium', 'value': 0.004000000189989805, '100g': 0.004000000189989805, 'serving': None, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'salt', 'value': 0.009999999776482582, '100g': 0.009999999776482582, 'serving': None, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'fat', 'value': 48.0, '100g': 48.0, 'serving': None, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'cocoa', 'value': 17.520000457763672, '100g': 17.520000457763672, 'serving': 17.520000457763672, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'carbohydrates', 'value': 36.0, '100g': 36.0, 'serving': None, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'proteins', 'value': 8.0, '100g': 8.0, 'serving': None, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'sugars', 'value': 32.0, '100g': 32.0, 'serving': None, 'unit': 'g', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'energy', 'value': 617.0, '100g': 2582.0, 'serving': None, 'unit': 'kcal', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'nutrition-score-fr', 'value': None, '100g': 25.0, 'serving': None, 'unit': None, 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}\n",
      " {'name': 'energy-kcal', 'value': 617.0, '100g': 617.0, 'serving': None, 'unit': 'kcal', 'prepared_value': None, 'prepared_100g': None, 'prepared_serving': None, 'prepared_unit': None}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 1. Print ALL 144 Columns\n",
    "parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "all_columns = parquet_file.schema.names\n",
    "\n",
    "print(f\"--- ALL {len(all_columns)} COLUMNS ---\")\n",
    "# Print in chunks of 10 so it's readable\n",
    "for i in range(0, len(all_columns), 10):\n",
    "    print(all_columns[i:i+10])\n",
    "\n",
    "# 2. Inspect the Data (The \"X-Ray\")\n",
    "# We will load just the first 3 rows of the ENTIRE file to see what's inside.\n",
    "print(\"\\n--- üîç INSPECTING FIRST ROW DATA ---\")\n",
    "df_sample = pd.read_parquet(PARQUET_FILE).head(1)\n",
    "\n",
    "# We iterate through the columns of the first row to find numbers\n",
    "for col in df_sample.columns:\n",
    "    val = df_sample.iloc[0][col]\n",
    "    # If the value is a dictionary or looks like nutrition data, print it\n",
    "    if isinstance(val, dict) or (isinstance(val, str) and \"energy\" in str(val)):\n",
    "        print(f\"\\nüëâ Column '{col}' contains: {val}\")\n",
    "        \n",
    "# Check specifically for a 'nutriments' column (standard for Open Food Facts)\n",
    "if 'nutriments' in df_sample.columns:\n",
    "    print(\"\\n‚úÖ FOUND 'nutriments' column! Content:\")\n",
    "    print(df_sample.iloc[0]['nutriments'])\n",
    "else:\n",
    "    print(\"\\n‚ùå 'nutriments' column NOT found in top level.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c76ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SCANNING FILE SCHEMA (Safe Mode)...\n",
      "File has 86 columns.\n",
      "   ‚úÖ Found 'barcode' as column: 'code'\n",
      "   ‚úÖ Found 'name' as column: 'name'\n",
      "   ‚úÖ Found 'brand' as column: 'brands'\n",
      "   ‚úÖ Found 'grade' as column: 'nutriscore_grade'\n",
      "   ‚úÖ Found 'source_info' as column: 'manufacturing_places'\n",
      "   ‚ö†Ô∏è WARNING: Could not find column for 'nutrition_blob'\n",
      "\n",
      "‚è≥ Loading 5 columns from Parquet...\n",
      "\n",
      "‚ùå STILL CRASHING? Error: No match for FieldRef.Name(name) in additives_n: int32\n",
      "additives_tags: list<element: string>\n",
      "allergens_tags: list<element: string>\n",
      "brands_tags: list<element: string>\n",
      "brands: string\n",
      "categories: string\n",
      "categories_tags: list<element: string>\n",
      "categories_properties: struct<ciqual_food_code: int32, agribalyse_food_code: int32, agribalyse_proxy_food_code: int32>\n",
      "checkers_tags: list<element: string>\n",
      "ciqual_food_name_tags: list<element: string>\n",
      "cities_tags: list<element: string>\n",
      "code: string\n",
      "compared_to_category: string\n",
      "complete: int32\n",
      "completeness: float\n",
      "correctors_tags: list<element: string>\n",
      "countries_tags: list<element: string>\n",
      "created_t: int64\n",
      "creator: string\n",
      "data_quality_errors_tags: list<element: string>\n",
      "data_quality_info_tags: list<element: string>\n",
      "data_quality_warnings_tags: list<element: string>\n",
      "data_sources_tags: list<element: string>\n",
      "ecoscore_data: string\n",
      "ecoscore_grade: string\n",
      "ecoscore_score: int32\n",
      "ecoscore_tags: list<element: string>\n",
      "editors: list<element: string>\n",
      "emb_codes_tags: list<element: string>\n",
      "emb_codes: string\n",
      "entry_dates_tags: list<element: string>\n",
      "food_groups_tags: list<element: string>\n",
      "generic_name: list<element: struct<lang: string, text: string>>\n",
      "images: list<element: struct<key: string, imgid: int32, rev: int32, sizes: struct<100: struct<h: int32, w: int32>, 200: struct<h: int32, w: int32>, 400: struct<h: int32, w: int32>, full: struct<h: int32, w: int32>>, uploaded_t: int64, uploader: string>>\n",
      "informers_tags: list<element: string>\n",
      "ingredients_analysis_tags: list<element: string>\n",
      "ingredients_from_palm_oil_n: int32\n",
      "ingredients_n: int32\n",
      "ingredients_original_tags: list<element: string>\n",
      "ingredients_percent_analysis: int32\n",
      "ingredients_tags: list<element: string>\n",
      "ingredients_text: list<element: struct<lang: string, text: string>>\n",
      "ingredients_with_specified_percent_n: int32\n",
      "ingredients_with_unspecified_percent_n: int32\n",
      "ingredients_without_ciqual_codes_n: int32\n",
      "ingredients_without_ciqual_codes: list<element: string>\n",
      "ingredients: string\n",
      "known_ingredients_n: int32\n",
      "labels_tags: list<element: string>\n",
      "labels: string\n",
      "lang: string\n",
      "languages_tags: list<element: string>\n",
      "last_edit_dates_tags: list<element: string>\n",
      "last_editor: string\n",
      "last_image_t: int64\n",
      "last_modified_by: string\n",
      "last_modified_t: int64\n",
      "last_updated_t: int64\n",
      "link: string\n",
      "main_countries_tags: list<element: string>\n",
      "manufacturing_places_tags: list<element: string>\n",
      "manufacturing_places: string\n",
      "max_imgid: int32\n",
      "minerals_tags: list<element: string>\n",
      "misc_tags: list<element: string>\n",
      "new_additives_n: int32\n",
      "no_nutrition_data: bool\n",
      "nova_group: int32\n",
      "nova_groups_tags: list<element: string>\n",
      "nova_groups: string\n",
      "nucleotides_tags: list<element: string>\n",
      "nutrient_levels_tags: list<element: string>\n",
      "nutriments: list<element: struct<name: string, value: float, 100g: float, serving: float, unit: string, prepared_value: float, prepared_100g: float, prepared_serving: float, prepared_unit: string>>\n",
      "nutriscore_grade: string\n",
      "nutriscore_score: int32\n",
      "nutrition_data_per: string\n",
      "obsolete: bool\n",
      "origins_tags: list<element: string>\n",
      "origins: string\n",
      "owner_fields: list<element: struct<field_name: string, timestamp: int64>>\n",
      "owner: string\n",
      "packagings_complete: bool\n",
      "packaging_recycling_tags: list<element: string>\n",
      "packaging_shapes_tags: list<element: string>\n",
      "packaging_tags: list<element: string>\n",
      "packaging_text: list<element: struct<lang: string, text: string>>\n",
      "packaging: string\n",
      "packagings: list<element: struct<material: string, number_of_units: int64, quantity_per_unit: string, quantity_per_unit_unit: string, quantity_per_unit_value: string, recycling: string, shape: string, weight_measured: float>>\n",
      "photographers: list<element: string>\n",
      "popularity_key: int64\n",
      "popularity_tags: list<element: string>\n",
      "product_name: list<element: struct<lang: string, text: string>>\n",
      "product_quantity_unit: string\n",
      "product_quantity: string\n",
      "purchase_places_tags: list<element: string>\n",
      "quantity: string\n",
      "rev: int32\n",
      "scans_n: int32\n",
      "serving_quantity: string\n",
      "serving_size: string\n",
      "states_tags: list<element: string>\n",
      "stores_tags: list<element: string>\n",
      "stores: string\n",
      "traces_tags: list<element: string>\n",
      "unique_scans_n: int32\n",
      "unknown_ingredients_n: int32\n",
      "unknown_nutrients_tags: list<element: string>\n",
      "vitamins_tags: list<element: string>\n",
      "with_non_nutritive_sweeteners: int32\n",
      "with_sweeteners: int32\n",
      "__fragment_index: int32\n",
      "__batch_index: int32\n",
      "__last_in_fragment: bool\n",
      "__filename: string\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "print(\"üîç SCANNING FILE SCHEMA (Safe Mode)...\")\n",
    "\n",
    "# 1. Get ALL available column names directly from the file\n",
    "parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "all_columns = set(parquet_file.schema.names) # Use a set for fast lookup\n",
    "\n",
    "print(f\"File has {len(all_columns)} columns.\")\n",
    "\n",
    "# 2. Define the columns we WANT (and their aliases)\n",
    "# We map 'Target Name' -> ['Possible Name 1', 'Possible Name 2']\n",
    "wanted_columns = {\n",
    "    'barcode': ['code', 'barcode', 'id'],\n",
    "    'name': ['product_name', 'name', 'generic_name'],\n",
    "    'brand': ['brands', 'brand', 'brand_owner'],\n",
    "    'grade': ['nutriscore_grade', 'nutrition_grades', 'nutrition_grade_fr'],\n",
    "    'source_info': ['manufacturing_places', 'origins', 'countries_tags'],\n",
    "    'nutrition_blob': ['nutriments', 'nutrition_data', 'nutriments_estimated']\n",
    "}\n",
    "\n",
    "# 3. Find the ACTUAL names in this specific file\n",
    "final_cols = []\n",
    "mapping = {} # To rename them later\n",
    "\n",
    "for target, possibilities in wanted_columns.items():\n",
    "    found = False\n",
    "    for candidate in possibilities:\n",
    "        if candidate in all_columns:\n",
    "            final_cols.append(candidate)\n",
    "            mapping[candidate] = target # Store how to rename it\n",
    "            print(f\"   ‚úÖ Found '{target}' as column: '{candidate}'\")\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Could not find column for '{target}'\")\n",
    "\n",
    "# 4. Load ONLY the columns we found\n",
    "print(f\"\\n‚è≥ Loading {len(final_cols)} columns from Parquet...\")\n",
    "try:\n",
    "    df_off = pd.read_parquet(PARQUET_FILE, columns=final_cols)\n",
    "    \n",
    "    # 5. Rename to standard names\n",
    "    df_off = df_off.rename(columns=mapping)\n",
    "    print(f\"Loaded {len(df_off)} rows.\")\n",
    "\n",
    "    # 6. Filter for INDIA\n",
    "    # We check 'source_info' (was manufacturing_places) for 'India'\n",
    "    # We also check 'brand' for common Indian brands\n",
    "    print(\"Filtering for Indian products...\")\n",
    "    \n",
    "    # Create a mask for filtering\n",
    "    mask = pd.Series(False, index=df_off.index)\n",
    "    \n",
    "    if 'source_info' in df_off.columns:\n",
    "        mask |= df_off['source_info'].str.contains('india', case=False, na=False)\n",
    "    \n",
    "    if 'brand' in df_off.columns:\n",
    "        mask |= df_off['brand'].str.contains('Amul|Britannia|Parle|Tata|Haldiram|Dabur|Nestle India', case=False, na=False)\n",
    "        \n",
    "    df_india = df_off[mask].copy()\n",
    "    print(f\"Filtered down to {len(df_india)} Indian products.\")\n",
    "\n",
    "    # 7. Extract Nutrition (The Hard Part)\n",
    "    # Only run this if we actually found the 'nutrition_blob' column\n",
    "    if 'nutrition_blob' in df_india.columns:\n",
    "        print(\"Extracting Sugar/Fat/Protein from nested data...\")\n",
    "        \n",
    "        def extract_val(blob, key_name):\n",
    "            # Safe extraction function\n",
    "            if isinstance(blob, list) or isinstance(blob, np.ndarray):\n",
    "                for item in blob:\n",
    "                    if isinstance(item, dict) and item.get('name') == key_name:\n",
    "                        # Return the value (prioritize 100g)\n",
    "                        return float(item.get('100g', item.get('value', 0.0)))\n",
    "            return 0.0\n",
    "\n",
    "        # Create new columns\n",
    "        df_india['sugar'] = df_india['nutrition_blob'].apply(lambda x: extract_val(x, 'sugars'))\n",
    "        df_india['fat'] = df_india['nutrition_blob'].apply(lambda x: extract_val(x, 'fat'))\n",
    "        df_india['protein'] = df_india['nutrition_blob'].apply(lambda x: extract_val(x, 'proteins'))\n",
    "        df_india['calories'] = df_india['nutrition_blob'].apply(lambda x: extract_val(x, 'energy-kcal'))\n",
    "        \n",
    "        # Drop the heavy blob column\n",
    "        df_india = df_india.drop(columns=['nutrition_blob'])\n",
    "    else:\n",
    "        print(\"‚ùå CRITICAL: No nutrition data found. Creating empty columns.\")\n",
    "        df_india['sugar'] = 0.0\n",
    "        df_india['fat'] = 0.0\n",
    "        df_india['protein'] = 0.0\n",
    "        df_india['calories'] = 0.0\n",
    "\n",
    "    # 8. Final Clean\n",
    "    if 'grade' in df_india.columns:\n",
    "        df_india['grade'] = df_india['grade'].str.upper()\n",
    "    \n",
    "    df_india['source'] = 'OpenFoodFacts'\n",
    "    \n",
    "    # Save\n",
    "    output_path = os.path.join(PROJECT_DIR, \"processed_off_india_safe.csv\")\n",
    "    df_india.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ SUCCESS! Saved to: {output_path}\")\n",
    "    display(df_india.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå STILL CRASHING? Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e46ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING BATCH PROCESSING (The Safe Way)...\n",
      "File found. Total Row Groups: 4190\n",
      "   Batch 1: Found 1 Indian items.\n",
      "   Batch 2: Found 7 Indian items.\n",
      "   Batch 3: Found 29 Indian items.\n",
      "   Batch 6: Found 7 Indian items.\n",
      "   Batch 7: Found 41 Indian items.\n",
      "   Batch 8: Found 24 Indian items.\n",
      "   Batch 9: Found 4 Indian items.\n",
      "   Batch 10: Found 3 Indian items.\n",
      "   Batch 11: Found 6 Indian items.\n",
      "   Batch 12: Found 2 Indian items.\n",
      "   Batch 13: Found 11 Indian items.\n",
      "   Batch 14: Found 13 Indian items.\n",
      "   Batch 15: Found 4 Indian items.\n",
      "   Batch 16: Found 8 Indian items.\n",
      "   Batch 17: Found 5 Indian items.\n",
      "   Batch 18: Found 5 Indian items.\n",
      "   Batch 19: Found 1 Indian items.\n",
      "   Batch 20: Found 44 Indian items.\n",
      "   Batch 21: Found 10 Indian items.\n",
      "   Batch 22: Found 3 Indian items.\n",
      "   Batch 23: Found 5 Indian items.\n",
      "   Batch 24: Found 575 Indian items.\n",
      "   Batch 25: Found 10 Indian items.\n",
      "   Batch 26: Found 49 Indian items.\n",
      "   Batch 27: Found 4 Indian items.\n",
      "   Batch 28: Found 2 Indian items.\n",
      "   Batch 30: Found 1 Indian items.\n",
      "   Batch 31: Found 9 Indian items.\n",
      "   Batch 32: Found 3 Indian items.\n",
      "   Batch 33: Found 2 Indian items.\n",
      "   Batch 34: Found 58 Indian items.\n",
      "   Batch 35: Found 12 Indian items.\n",
      "   Batch 36: Found 2 Indian items.\n",
      "   Batch 37: Found 2 Indian items.\n",
      "   Batch 38: Found 1 Indian items.\n",
      "   Batch 39: Found 4 Indian items.\n",
      "   Batch 40: Found 11 Indian items.\n",
      "   Batch 41: Found 1 Indian items.\n",
      "   Batch 43: Found 60 Indian items.\n",
      "   Batch 44: Found 114 Indian items.\n",
      "   Batch 45: Found 22 Indian items.\n",
      "   Batch 46: Found 48 Indian items.\n",
      "   Batch 47: Found 100 Indian items.\n",
      "   Batch 48: Found 18 Indian items.\n",
      "   Batch 49: Found 23 Indian items.\n",
      "   Batch 50: Found 166 Indian items.\n",
      "   (Stopping early for test run...)\n",
      "\n",
      "‚úÖ Merging found products...\n",
      "‚ùå Error during batching: name 'PROJECT_DIR' is not defined\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"üöÄ STARTING BATCH PROCESSING (The Safe Way)...\")\n",
    "\n",
    "# 1. Open the Parquet file stream\n",
    "parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "print(f\"File found. Total Row Groups: {parquet_file.num_row_groups}\")\n",
    "\n",
    "# 2. Define lists to hold our Indian products\n",
    "indian_products = []\n",
    "batch_count = 0\n",
    "\n",
    "# 3. Iterate through the file in batches (chunks)\n",
    "# This prevents RAM crashes and bypasses \"Column Missing\" errors\n",
    "try:\n",
    "    for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Convert the batch to a Pandas DataFrame\n",
    "        df_chunk = batch.to_pandas()\n",
    "        \n",
    "        # --- DYNAMIC COLUMN MAPPING (Finds columns automatically) ---\n",
    "        # We look for ANY column that looks like 'country' or 'place'\n",
    "        country_col = next((c for c in df_chunk.columns if 'countr' in c.lower() or 'place' in c.lower()), None)\n",
    "        brand_col = next((c for c in df_chunk.columns if 'brand' in c.lower()), None)\n",
    "        \n",
    "        # If we can't find a country column, we can't filter safely, so we skip\n",
    "        if not country_col and not brand_col:\n",
    "            continue\n",
    "\n",
    "        # --- FILTERING FOR INDIA ---\n",
    "        mask = pd.Series(False, index=df_chunk.index)\n",
    "        \n",
    "        # Check Country/Place column\n",
    "        if country_col:\n",
    "            mask |= df_chunk[country_col].astype(str).str.contains('india', case=False, na=False)\n",
    "            \n",
    "        # Check Brand column (Backup filter)\n",
    "        if brand_col:\n",
    "            mask |= df_chunk[brand_col].astype(str).str.contains('Amul|Britannia|Parle|Tata|Haldiram|Nestle', case=False, na=False)\n",
    "        \n",
    "        # Apply filter\n",
    "        df_india_chunk = df_chunk[mask].copy()\n",
    "        \n",
    "        # If we found Indian products in this chunk, save them\n",
    "        if not df_india_chunk.empty:\n",
    "            print(f\"   Batch {batch_count}: Found {len(df_india_chunk)} Indian items.\")\n",
    "            indian_products.append(df_india_chunk)\n",
    "            \n",
    "        # Stop after checking 50 batches (approx 500k rows) just to test\n",
    "        # Remove this break if you want to scan the WHOLE file (takes longer)\n",
    "        if batch_count >= 50: \n",
    "            print(\"   (Stopping early for test run...)\")\n",
    "            break\n",
    "\n",
    "    # 4. MERGE & SAVE\n",
    "    if indian_products:\n",
    "        print(\"\\n‚úÖ Merging found products...\")\n",
    "        final_df = pd.concat(indian_products, ignore_index=True)\n",
    "        \n",
    "        # NOW we select columns (Safety Check: only keep columns that actually exist)\n",
    "        keep_cols = [c for c in final_df.columns if c in ['code', 'product_name', 'brands', 'nutriscore_grade', 'image_url']]\n",
    "        final_df = final_df[keep_cols]\n",
    "        \n",
    "        # Rename for clarity\n",
    "        rename_map = {\n",
    "            'code': 'barcode',\n",
    "            'product_name': 'name', \n",
    "            'brands': 'brand',\n",
    "            'nutriscore_grade': 'grade',\n",
    "            'image_url': 'image'\n",
    "        }\n",
    "        final_df = final_df.rename(columns=rename_map)\n",
    "        \n",
    "        # Add Source\n",
    "        final_df['source'] = 'OpenFoodFacts'\n",
    "        \n",
    "        # Save\n",
    "        output_path = os.path.join(PROJECT_DIR, \"india_products_batch_v1.csv\")\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"üéâ SUCCESS! Saved {len(final_df)} Indian products to: {output_path}\")\n",
    "        display(final_df.head())\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No Indian products found in the first 50 batches.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during batching: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1267f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING ROBUST FINAL PROCESSING...\n",
      "File found. Scanning in batches...\n",
      "   Batch 1: Found 1 items.\n",
      "   Batch 2: Found 7 items.\n",
      "   Batch 3: Found 29 items.\n",
      "   Batch 6: Found 7 items.\n",
      "   Batch 7: Found 41 items.\n",
      "   Batch 8: Found 24 items.\n",
      "   Batch 9: Found 4 items.\n",
      "   Batch 10: Found 3 items.\n",
      "   Batch 11: Found 6 items.\n",
      "   Batch 12: Found 2 items.\n",
      "   Batch 13: Found 11 items.\n",
      "   Batch 14: Found 13 items.\n",
      "   Batch 15: Found 4 items.\n",
      "   Batch 16: Found 8 items.\n",
      "   Batch 17: Found 5 items.\n",
      "   Batch 18: Found 5 items.\n",
      "   Batch 19: Found 1 items.\n",
      "   Batch 20: Found 44 items.\n",
      "   Batch 21: Found 10 items.\n",
      "   Batch 22: Found 3 items.\n",
      "   Batch 23: Found 5 items.\n",
      "   Batch 24: Found 575 items.\n",
      "   Batch 25: Found 10 items.\n",
      "   Batch 26: Found 49 items.\n",
      "   Batch 27: Found 4 items.\n",
      "   Batch 28: Found 2 items.\n",
      "   Batch 30: Found 1 items.\n",
      "   Batch 31: Found 9 items.\n",
      "   Batch 32: Found 3 items.\n",
      "   Batch 33: Found 2 items.\n",
      "   Batch 34: Found 58 items.\n",
      "   Batch 35: Found 12 items.\n",
      "   Batch 36: Found 2 items.\n",
      "   Batch 37: Found 2 items.\n",
      "   Batch 38: Found 1 items.\n",
      "   Batch 39: Found 4 items.\n",
      "   Batch 40: Found 11 items.\n",
      "   Batch 41: Found 1 items.\n",
      "   Batch 43: Found 60 items.\n",
      "   Batch 44: Found 114 items.\n",
      "   Batch 45: Found 22 items.\n",
      "   Batch 46: Found 48 items.\n",
      "   Batch 47: Found 103 items.\n",
      "   Batch 48: Found 18 items.\n",
      "   Batch 49: Found 23 items.\n",
      "   Batch 50: Found 166 items.\n",
      "   Batch 51: Found 113 items.\n",
      "   Batch 52: Found 794 items.\n",
      "   Batch 53: Found 1219 items.\n",
      "   Batch 54: Found 414 items.\n",
      "   Batch 55: Found 15 items.\n",
      "   Batch 56: Found 98 items.\n",
      "   Batch 57: Found 28 items.\n",
      "   Batch 58: Found 87 items.\n",
      "   Batch 59: Found 383 items.\n",
      "   Batch 60: Found 126 items.\n",
      "   Batch 61: Found 99 items.\n",
      "   Batch 62: Found 92 items.\n",
      "   Batch 63: Found 72 items.\n",
      "   Batch 64: Found 89 items.\n",
      "   Batch 65: Found 63 items.\n",
      "   Batch 66: Found 71 items.\n",
      "   Batch 67: Found 66 items.\n",
      "   Batch 68: Found 78 items.\n",
      "   Batch 69: Found 64 items.\n",
      "   Batch 70: Found 58 items.\n",
      "   Batch 71: Found 70 items.\n",
      "   Batch 72: Found 70 items.\n",
      "   Batch 73: Found 59 items.\n",
      "   Batch 74: Found 72 items.\n",
      "   Batch 75: Found 57 items.\n",
      "   Batch 76: Found 62 items.\n",
      "   Batch 77: Found 96 items.\n",
      "   Batch 78: Found 81 items.\n",
      "   Batch 79: Found 67 items.\n",
      "   Batch 80: Found 92 items.\n",
      "   Batch 81: Found 82 items.\n",
      "   Batch 82: Found 85 items.\n",
      "   Batch 83: Found 81 items.\n",
      "   Batch 84: Found 65 items.\n",
      "   Batch 85: Found 86 items.\n",
      "   Batch 86: Found 97 items.\n",
      "   Batch 87: Found 105 items.\n",
      "   Batch 88: Found 97 items.\n",
      "   Batch 89: Found 87 items.\n",
      "   Batch 90: Found 81 items.\n",
      "   Batch 91: Found 76 items.\n",
      "   Batch 92: Found 73 items.\n",
      "   Batch 93: Found 55 items.\n",
      "   Batch 94: Found 64 items.\n",
      "   Batch 95: Found 58 items.\n",
      "   Batch 96: Found 71 items.\n",
      "   Batch 97: Found 52 items.\n",
      "   Batch 98: Found 68 items.\n",
      "   Batch 99: Found 72 items.\n",
      "   Batch 100: Found 73 items.\n",
      "   Batch 101: Found 67 items.\n",
      "   Batch 102: Found 62 items.\n",
      "   Batch 103: Found 62 items.\n",
      "   Batch 104: Found 55 items.\n",
      "   Batch 105: Found 55 items.\n",
      "   Batch 106: Found 51 items.\n",
      "   Batch 107: Found 51 items.\n",
      "   Batch 108: Found 50 items.\n",
      "   Batch 109: Found 64 items.\n",
      "   Batch 110: Found 43 items.\n",
      "   Batch 111: Found 57 items.\n",
      "   Batch 112: Found 55 items.\n",
      "   Batch 113: Found 47 items.\n",
      "   Batch 114: Found 71 items.\n",
      "   Batch 115: Found 85 items.\n",
      "   Batch 116: Found 74 items.\n",
      "   Batch 117: Found 83 items.\n",
      "   Batch 118: Found 73 items.\n",
      "   Batch 119: Found 2 items.\n",
      "   Batch 120: Found 2 items.\n",
      "   Batch 121: Found 2 items.\n",
      "   Batch 122: Found 26 items.\n",
      "   Batch 123: Found 1 items.\n",
      "   Batch 125: Found 4 items.\n",
      "   Batch 126: Found 2 items.\n",
      "   Batch 127: Found 1 items.\n",
      "   Batch 128: Found 2 items.\n",
      "   Batch 129: Found 20 items.\n",
      "   Batch 130: Found 3 items.\n",
      "   Batch 131: Found 50 items.\n",
      "   Batch 132: Found 55 items.\n",
      "   Batch 133: Found 78 items.\n",
      "   Batch 134: Found 74 items.\n",
      "   Batch 135: Found 71 items.\n",
      "   Batch 136: Found 70 items.\n",
      "   Batch 137: Found 70 items.\n",
      "   Batch 138: Found 55 items.\n",
      "   Batch 139: Found 56 items.\n",
      "   Batch 140: Found 53 items.\n",
      "   Batch 141: Found 60 items.\n",
      "   Batch 142: Found 60 items.\n",
      "   Batch 143: Found 58 items.\n",
      "   Batch 144: Found 62 items.\n",
      "   Batch 145: Found 67 items.\n",
      "   Batch 146: Found 62 items.\n",
      "   Batch 147: Found 66 items.\n",
      "   Batch 148: Found 61 items.\n",
      "   Batch 149: Found 57 items.\n",
      "   Batch 150: Found 52 items.\n",
      "   Batch 151: Found 43 items.\n",
      "   Batch 152: Found 53 items.\n",
      "   Batch 153: Found 49 items.\n",
      "   Batch 154: Found 38 items.\n",
      "   Batch 155: Found 41 items.\n",
      "   Batch 156: Found 30 items.\n",
      "   Batch 157: Found 43 items.\n",
      "   Batch 158: Found 51 items.\n",
      "   Batch 159: Found 57 items.\n",
      "   Batch 160: Found 55 items.\n",
      "   Batch 161: Found 37 items.\n",
      "   Batch 162: Found 40 items.\n",
      "   Batch 163: Found 60 items.\n",
      "   Batch 164: Found 36 items.\n",
      "   Batch 165: Found 66 items.\n",
      "   Batch 166: Found 42 items.\n",
      "   Batch 167: Found 53 items.\n",
      "   Batch 168: Found 45 items.\n",
      "   Batch 169: Found 49 items.\n",
      "   Batch 170: Found 59 items.\n",
      "   Batch 171: Found 67 items.\n",
      "   Batch 172: Found 53 items.\n",
      "   Batch 173: Found 65 items.\n",
      "   Batch 174: Found 57 items.\n",
      "   Batch 175: Found 56 items.\n",
      "   Batch 176: Found 54 items.\n",
      "   Batch 177: Found 77 items.\n",
      "   Batch 178: Found 62 items.\n",
      "   Batch 179: Found 48 items.\n",
      "   Batch 180: Found 63 items.\n",
      "   Batch 181: Found 34 items.\n",
      "   Batch 182: Found 47 items.\n",
      "   Batch 183: Found 46 items.\n",
      "   Batch 184: Found 65 items.\n",
      "   Batch 185: Found 49 items.\n",
      "   Batch 186: Found 51 items.\n",
      "   Batch 187: Found 52 items.\n",
      "   Batch 188: Found 45 items.\n",
      "   Batch 189: Found 26 items.\n",
      "   Batch 190: Found 55 items.\n",
      "   Batch 191: Found 60 items.\n",
      "   Batch 192: Found 62 items.\n",
      "   Batch 193: Found 49 items.\n",
      "   Batch 194: Found 52 items.\n",
      "   Batch 195: Found 49 items.\n",
      "   Batch 196: Found 31 items.\n",
      "   Batch 197: Found 76 items.\n",
      "   Batch 198: Found 50 items.\n",
      "   Batch 199: Found 42 items.\n",
      "   Batch 200: Found 61 items.\n",
      "   Batch 201: Found 41 items.\n",
      "   Batch 202: Found 40 items.\n",
      "   Batch 203: Found 31 items.\n",
      "   Batch 204: Found 55 items.\n",
      "   Batch 205: Found 41 items.\n",
      "   Batch 206: Found 58 items.\n",
      "   Batch 207: Found 36 items.\n",
      "   Batch 208: Found 35 items.\n",
      "   Batch 209: Found 60 items.\n",
      "   Batch 210: Found 41 items.\n",
      "   Batch 211: Found 45 items.\n",
      "   Batch 212: Found 51 items.\n",
      "   Batch 213: Found 52 items.\n",
      "   Batch 214: Found 55 items.\n",
      "   Batch 215: Found 60 items.\n",
      "   Batch 216: Found 60 items.\n",
      "   Batch 217: Found 64 items.\n",
      "   Batch 218: Found 63 items.\n",
      "   Batch 219: Found 91 items.\n",
      "   Batch 220: Found 57 items.\n",
      "   Batch 221: Found 63 items.\n",
      "   Batch 222: Found 61 items.\n",
      "   Batch 223: Found 106 items.\n",
      "   Batch 224: Found 62 items.\n",
      "   Batch 225: Found 63 items.\n",
      "   Batch 226: Found 61 items.\n",
      "   Batch 227: Found 53 items.\n",
      "   Batch 228: Found 65 items.\n",
      "   Batch 229: Found 38 items.\n",
      "   Batch 230: Found 71 items.\n",
      "   Batch 231: Found 55 items.\n",
      "   Batch 232: Found 58 items.\n",
      "   Batch 233: Found 52 items.\n",
      "   Batch 234: Found 96 items.\n",
      "   Batch 235: Found 45 items.\n",
      "   Batch 236: Found 46 items.\n",
      "   Batch 237: Found 63 items.\n",
      "   Batch 238: Found 41 items.\n",
      "   Batch 239: Found 54 items.\n",
      "   Batch 240: Found 32 items.\n",
      "   Batch 241: Found 53 items.\n",
      "   Batch 242: Found 58 items.\n",
      "   Batch 243: Found 60 items.\n",
      "   Batch 244: Found 54 items.\n",
      "   Batch 245: Found 49 items.\n",
      "   Batch 246: Found 61 items.\n",
      "   Batch 247: Found 60 items.\n",
      "   Batch 248: Found 79 items.\n",
      "   Batch 249: Found 61 items.\n",
      "   Batch 250: Found 66 items.\n",
      "   Batch 251: Found 59 items.\n",
      "   Batch 252: Found 56 items.\n",
      "   Batch 253: Found 86 items.\n",
      "   Batch 254: Found 77 items.\n",
      "   Batch 255: Found 95 items.\n",
      "   Batch 256: Found 69 items.\n",
      "   Batch 257: Found 72 items.\n",
      "   Batch 258: Found 84 items.\n",
      "   Batch 259: Found 67 items.\n",
      "   Batch 260: Found 73 items.\n",
      "   Batch 261: Found 57 items.\n",
      "   Batch 262: Found 47 items.\n",
      "   Batch 263: Found 48 items.\n",
      "   Batch 264: Found 61 items.\n",
      "   Batch 265: Found 51 items.\n",
      "   Batch 266: Found 77 items.\n",
      "   Batch 267: Found 86 items.\n",
      "   Batch 268: Found 60 items.\n",
      "   Batch 269: Found 93 items.\n",
      "   Batch 270: Found 91 items.\n",
      "   Batch 271: Found 103 items.\n",
      "   Batch 272: Found 84 items.\n",
      "   Batch 273: Found 76 items.\n",
      "   Batch 274: Found 79 items.\n",
      "   Batch 275: Found 93 items.\n",
      "   Batch 276: Found 89 items.\n",
      "   Batch 277: Found 72 items.\n",
      "   Batch 278: Found 98 items.\n",
      "   Batch 279: Found 103 items.\n",
      "   Batch 280: Found 239 items.\n",
      "   Batch 281: Found 152 items.\n",
      "   Batch 282: Found 112 items.\n",
      "   Batch 283: Found 161 items.\n",
      "   Batch 284: Found 98 items.\n",
      "   Batch 285: Found 309 items.\n",
      "   Batch 286: Found 110 items.\n",
      "   Batch 287: Found 99 items.\n",
      "   Batch 288: Found 126 items.\n",
      "   Batch 289: Found 190 items.\n",
      "   Batch 290: Found 105 items.\n",
      "   Batch 291: Found 177 items.\n",
      "   Batch 292: Found 69 items.\n",
      "   Batch 293: Found 75 items.\n",
      "   Batch 294: Found 81 items.\n",
      "   Batch 295: Found 63 items.\n",
      "   Batch 296: Found 57 items.\n",
      "   Batch 297: Found 99 items.\n",
      "   Batch 298: Found 146 items.\n",
      "   Batch 299: Found 176 items.\n",
      "   Batch 300: Found 132 items.\n",
      "   Batch 301: Found 134 items.\n",
      "   Batch 302: Found 173 items.\n",
      "   Batch 303: Found 254 items.\n",
      "   Batch 304: Found 140 items.\n",
      "   Batch 305: Found 166 items.\n",
      "   Batch 306: Found 116 items.\n",
      "   Batch 307: Found 117 items.\n",
      "   Batch 308: Found 159 items.\n",
      "   Batch 309: Found 253 items.\n",
      "   Batch 310: Found 236 items.\n",
      "   Batch 311: Found 180 items.\n",
      "   Batch 312: Found 133 items.\n",
      "   Batch 313: Found 170 items.\n",
      "   Batch 314: Found 87 items.\n",
      "   Batch 315: Found 3 items.\n",
      "   Batch 317: Found 33 items.\n",
      "   Batch 318: Found 144 items.\n",
      "   Batch 319: Found 118 items.\n",
      "   Batch 320: Found 133 items.\n",
      "   Batch 321: Found 86 items.\n",
      "   Batch 322: Found 93 items.\n",
      "   Batch 323: Found 219 items.\n",
      "   Batch 324: Found 154 items.\n",
      "   Batch 325: Found 167 items.\n",
      "   Batch 326: Found 88 items.\n",
      "   Batch 327: Found 133 items.\n",
      "   Batch 328: Found 75 items.\n",
      "   Batch 329: Found 117 items.\n",
      "   Batch 330: Found 100 items.\n",
      "   Batch 331: Found 86 items.\n",
      "   Batch 332: Found 105 items.\n",
      "   Batch 333: Found 169 items.\n",
      "   Batch 334: Found 64 items.\n",
      "   Batch 335: Found 24 items.\n",
      "   Batch 336: Found 116 items.\n",
      "   Batch 337: Found 100 items.\n",
      "   Batch 338: Found 168 items.\n",
      "   Batch 339: Found 71 items.\n",
      "   Batch 341: Found 122 items.\n",
      "   Batch 342: Found 122 items.\n",
      "   Batch 343: Found 102 items.\n",
      "   Batch 344: Found 110 items.\n",
      "   Batch 345: Found 149 items.\n",
      "   Batch 346: Found 146 items.\n",
      "   Batch 347: Found 175 items.\n",
      "   Batch 348: Found 173 items.\n",
      "   Batch 349: Found 185 items.\n",
      "   Batch 350: Found 111 items.\n",
      "   Batch 351: Found 123 items.\n",
      "   Batch 352: Found 135 items.\n",
      "   Batch 353: Found 107 items.\n",
      "   Batch 354: Found 79 items.\n",
      "   Batch 355: Found 82 items.\n",
      "   Batch 356: Found 74 items.\n",
      "   Batch 357: Found 113 items.\n",
      "   Batch 358: Found 86 items.\n",
      "   Batch 359: Found 87 items.\n",
      "   Batch 360: Found 135 items.\n",
      "   Batch 361: Found 95 items.\n",
      "   Batch 362: Found 100 items.\n",
      "   Batch 363: Found 113 items.\n",
      "   Batch 365: Found 1 items.\n",
      "   Batch 366: Found 16 items.\n",
      "   Batch 367: Found 53 items.\n",
      "   Batch 368: Found 131 items.\n",
      "   Batch 369: Found 128 items.\n",
      "   Batch 370: Found 128 items.\n",
      "   Batch 371: Found 137 items.\n",
      "   Batch 372: Found 139 items.\n",
      "   Batch 373: Found 229 items.\n",
      "   Batch 374: Found 137 items.\n",
      "   Batch 375: Found 130 items.\n",
      "   Batch 376: Found 96 items.\n",
      "   Batch 377: Found 125 items.\n",
      "   Batch 378: Found 107 items.\n",
      "   Batch 379: Found 35 items.\n",
      "   Batch 380: Found 115 items.\n",
      "   Batch 381: Found 80 items.\n",
      "   Batch 382: Found 100 items.\n",
      "   Batch 383: Found 96 items.\n",
      "   Batch 384: Found 159 items.\n",
      "   Batch 385: Found 199 items.\n",
      "   Batch 386: Found 633 items.\n",
      "   Batch 387: Found 314 items.\n",
      "   Batch 388: Found 279 items.\n",
      "   Batch 389: Found 189 items.\n",
      "   Batch 390: Found 177 items.\n",
      "   Batch 391: Found 174 items.\n",
      "   Batch 392: Found 190 items.\n",
      "   Batch 393: Found 173 items.\n",
      "   Batch 394: Found 158 items.\n",
      "   Batch 395: Found 115 items.\n",
      "   Batch 396: Found 213 items.\n",
      "   Batch 397: Found 105 items.\n",
      "   Batch 398: Found 168 items.\n",
      "   Batch 399: Found 112 items.\n",
      "   Batch 400: Found 85 items.\n",
      "   Batch 401: Found 126 items.\n",
      "   Batch 402: Found 118 items.\n",
      "   Batch 403: Found 110 items.\n",
      "   Batch 404: Found 131 items.\n",
      "   Batch 405: Found 122 items.\n",
      "   Batch 406: Found 112 items.\n",
      "   Batch 407: Found 102 items.\n",
      "   Batch 408: Found 86 items.\n",
      "   Batch 409: Found 134 items.\n",
      "   Batch 410: Found 76 items.\n",
      "   Batch 412: Found 29 items.\n",
      "   Batch 413: Found 106 items.\n",
      "   Batch 414: Found 221 items.\n",
      "   Batch 415: Found 174 items.\n",
      "   Batch 416: Found 110 items.\n",
      "   Batch 417: Found 125 items.\n",
      "   Batch 418: Found 60 items.\n",
      "   Batch 419: Found 104 items.\n",
      "   Batch 420: Found 77 items.\n",
      "   Batch 421: Found 137 items.\n",
      "   Batch 422: Found 99 items.\n",
      "   Batch 423: Found 120 items.\n",
      "   Batch 424: Found 76 items.\n",
      "   Batch 425: Found 90 items.\n",
      "   Batch 426: Found 57 items.\n",
      "   Batch 427: Found 67 items.\n",
      "   Batch 428: Found 73 items.\n",
      "   Batch 429: Found 155 items.\n",
      "   Batch 430: Found 7 items.\n",
      "\n",
      "‚úÖ Merging datasets...\n",
      "üéâ SUCCESS! Saved 36616 Indian products to:\n",
      "üìÇ C:\\Users\\arjun\\Downloads\\Nutri Rate\\india_products_final.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barcode</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>grade</th>\n",
       "      <th>sugar</th>\n",
       "      <th>fat</th>\n",
       "      <th>protein</th>\n",
       "      <th>calories</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00024907</td>\n",
       "      <td>[{'lang': 'main', 'text': 'Teriyaki Dip Sauce ...</td>\n",
       "      <td>Marks &amp; Spencer</td>\n",
       "      <td>e</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0012000713019</td>\n",
       "      <td>[]</td>\n",
       "      <td>Nestl√©</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0012345678905</td>\n",
       "      <td>[{'lang': 'main', 'text': 'Sph√®re plaisir'}, {...</td>\n",
       "      <td>Alte√±a's Nopales</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0013800143310</td>\n",
       "      <td>[{'lang': 'main', 'text': 'Italiano lasagna'},...</td>\n",
       "      <td>Nestl√©</td>\n",
       "      <td>c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0015000046446</td>\n",
       "      <td>[{'lang': 'main', 'text': 'Gerber Fruit &amp; Yogu...</td>\n",
       "      <td>Nestl√©</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         barcode                                               name  \\\n",
       "0       00024907  [{'lang': 'main', 'text': 'Teriyaki Dip Sauce ...   \n",
       "1  0012000713019                                                 []   \n",
       "2  0012345678905  [{'lang': 'main', 'text': 'Sph√®re plaisir'}, {...   \n",
       "3  0013800143310  [{'lang': 'main', 'text': 'Italiano lasagna'},...   \n",
       "4  0015000046446  [{'lang': 'main', 'text': 'Gerber Fruit & Yogu...   \n",
       "\n",
       "              brand    grade  sugar  fat  protein  calories         source  \n",
       "0   Marks & Spencer        e    0.0  0.0      0.0       0.0  OpenFoodFacts  \n",
       "1            Nestl√©  unknown    0.0  0.0      0.0       0.0  OpenFoodFacts  \n",
       "2  Alte√±a's Nopales  unknown    0.0  0.0      0.0       0.0  OpenFoodFacts  \n",
       "3            Nestl√©        c    0.0  0.0      0.0       0.0  OpenFoodFacts  \n",
       "4            Nestl√©  unknown    0.0  0.0      0.0       0.0  OpenFoodFacts  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP PATHS ---\n",
    "PROJECT_DIR = r\"C:\\Users\\arjun\\Downloads\\Nutri Rate\"\n",
    "PARQUET_FILE = r\"C:\\Users\\arjun\\Downloads\\food.parquet\"\n",
    "\n",
    "print(\"üöÄ STARTING ROBUST FINAL PROCESSING...\")\n",
    "\n",
    "# --- 2. THE FIXED EXTRACTION FUNCTION ---\n",
    "def extract_nutrient(nutriments_data, target_name):\n",
    "    \"\"\"\n",
    "    Safely extracts value. Handles None, 'None', and missing keys.\n",
    "    \"\"\"\n",
    "    # 1. Check if the input is a valid list/array\n",
    "    if not isinstance(nutriments_data, (list, np.ndarray)):\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Loop through the list\n",
    "    for item in nutriments_data:\n",
    "        if isinstance(item, dict) and item.get('name') == target_name:\n",
    "            # 3. Try to get '100g' first, then 'value'\n",
    "            val = item.get('100g')\n",
    "            if val is None:\n",
    "                val = item.get('value')\n",
    "            \n",
    "            # 4. If we still have None, return 0.0\n",
    "            if val is None:\n",
    "                return 0.0\n",
    "                \n",
    "            # 5. Try converting to float safely\n",
    "            try:\n",
    "                return float(val)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0.0\n",
    "                \n",
    "    return 0.0\n",
    "\n",
    "# --- 3. BATCH PROCESS ---\n",
    "parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "indian_products = []\n",
    "batch_count = 0\n",
    "\n",
    "print(f\"File found. Scanning in batches...\")\n",
    "\n",
    "# Iterate through file\n",
    "for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "    batch_count += 1\n",
    "    \n",
    "    df_chunk = batch.to_pandas()\n",
    "    \n",
    "    # A. Dynamic Column Search\n",
    "    country_col = next((c for c in df_chunk.columns if 'countr' in c.lower() or 'place' in c.lower()), None)\n",
    "    brand_col = next((c for c in df_chunk.columns if 'brand' in c.lower()), None)\n",
    "    # Search for 'nutriments' or 'nutrition_data' or 'nutrition_grades'\n",
    "    nutri_col = next((c for c in df_chunk.columns if 'nutriment' in c.lower() or 'nutrition_data' in c.lower()), None)\n",
    "\n",
    "    # B. Filter for India\n",
    "    mask = pd.Series(False, index=df_chunk.index)\n",
    "    if country_col:\n",
    "        mask |= df_chunk[country_col].astype(str).str.contains('india', case=False, na=False)\n",
    "    if brand_col:\n",
    "        mask |= df_chunk[brand_col].astype(str).str.contains('Amul|Britannia|Parle|Tata|Haldiram|Nestle|Dabur', case=False, na=False)\n",
    "    \n",
    "    df_india_chunk = df_chunk[mask].copy()\n",
    "\n",
    "    # C. Process Data\n",
    "    if not df_india_chunk.empty:\n",
    "        # Extract Nutrition using the FIXED function\n",
    "        if nutri_col:\n",
    "            # We use lambda to apply the function row by row\n",
    "            df_india_chunk['sugar'] = df_india_chunk[nutri_col].apply(lambda x: extract_nutrient(x, 'sugars'))\n",
    "            df_india_chunk['fat'] = df_india_chunk[nutri_col].apply(lambda x: extract_nutrient(x, 'fat'))\n",
    "            df_india_chunk['protein'] = df_india_chunk[nutri_col].apply(lambda x: extract_nutrient(x, 'proteins'))\n",
    "            df_india_chunk['calories'] = df_india_chunk[nutri_col].apply(lambda x: extract_nutrient(x, 'energy-kcal'))\n",
    "        else:\n",
    "            df_india_chunk['sugar'] = 0.0\n",
    "            df_india_chunk['fat'] = 0.0\n",
    "            df_india_chunk['protein'] = 0.0\n",
    "            df_india_chunk['calories'] = 0.0\n",
    "\n",
    "        print(f\"   Batch {batch_count}: Found {len(df_india_chunk)} items.\")\n",
    "        indian_products.append(df_india_chunk)\n",
    "\n",
    "    # Limit to 500 batches for a good sample (Remove this logic to scan EVERYTHING)\n",
    "    if batch_count >= 500: \n",
    "        print(\"   (Stopping at 500 batches...)\")\n",
    "        break\n",
    "\n",
    "# --- 4. MERGE & SAVE ---\n",
    "if indian_products:\n",
    "    print(\"\\n‚úÖ Merging datasets...\")\n",
    "    final_df = pd.concat(indian_products, ignore_index=True)\n",
    "    \n",
    "    # Clean Column Names\n",
    "    rename_map = {\n",
    "        'code': 'barcode', \n",
    "        'product_name': 'name', \n",
    "        'brands': 'brand', \n",
    "        'nutriscore_grade': 'grade',\n",
    "        'image_url': 'image'\n",
    "    }\n",
    "    # Only rename columns that exist\n",
    "    final_df = final_df.rename(columns=rename_map)\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    cols_we_want = ['barcode', 'name', 'brand', 'grade', 'sugar', 'fat', 'protein', 'calories', 'image']\n",
    "    final_cols = [c for c in cols_we_want if c in final_df.columns]\n",
    "    \n",
    "    final_df = final_df[final_cols]\n",
    "    final_df['source'] = 'OpenFoodFacts'\n",
    "\n",
    "    # Save\n",
    "    output_path = os.path.join(PROJECT_DIR, \"india_products_final.csv\")\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"üéâ SUCCESS! Saved {len(final_df)} Indian products to:\")\n",
    "    print(f\"üìÇ {output_path}\")\n",
    "    display(final_df.head())\n",
    "else:\n",
    "    print(\"‚ùå No Indian products found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2130ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ CLEANING DATA NAMES & VALUES...\n",
      "Fixing Names... (This is fast)\n",
      "Original Count: 36616\n",
      "Cleaned Count: 0 (Removed 36616 empty rows)\n",
      "‚ú® DONE! Your Golden Database is at: C:\\Users\\arjun\\Downloads\\Nutri Rate\\india_products_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barcode</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>grade</th>\n",
       "      <th>sugar</th>\n",
       "      <th>fat</th>\n",
       "      <th>protein</th>\n",
       "      <th>calories</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [barcode, name, brand, grade, sugar, fat, protein, calories, source]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "print(\"üßπ CLEANING DATA NAMES & VALUES...\")\n",
    "\n",
    "# 1. Load the file we just made (if not already in memory)\n",
    "# final_df = pd.read_csv(os.path.join(PROJECT_DIR, \"india_products_final.csv\")) \n",
    "\n",
    "# 2. Function to fix the Name column\n",
    "def fix_product_name(val):\n",
    "    try:\n",
    "        # If it's a list (already parsed), take the first item's text\n",
    "        if isinstance(val, (list, np.ndarray)) and len(val) > 0:\n",
    "             if isinstance(val[0], dict):\n",
    "                return val[0].get('text', 'Unknown Product')\n",
    "        \n",
    "        # If it's a string looking like a list \"[{'text':...}]\"\n",
    "        if isinstance(val, str) and val.startswith(\"[\"):\n",
    "            # specific fix for the format seen in your output\n",
    "            # We simple-parse it or regex it if ast fails, but let's try basic string slicing first\n",
    "            # \" ... 'text': 'REAL NAME' ... \"\n",
    "            import re\n",
    "            match = re.search(r\"'text':\\s*'([^']*)'\", val)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "                \n",
    "    except Exception:\n",
    "        pass\n",
    "    return val # Return original if we can't fix it\n",
    "\n",
    "print(\"Fixing Names... (This is fast)\")\n",
    "final_df['name'] = final_df['name'].apply(fix_product_name)\n",
    "\n",
    "# 3. Filter out \"Ghost\" Products (Where ALL nutrition is 0)\n",
    "# If Calories + Sugar + Fat + Protein == 0, the data is likely missing.\n",
    "# We keep them but mark them as \"Needs Scan\" or drop them. \n",
    "# Let's drop them to keep the database high-quality.\n",
    "print(f\"Original Count: {len(final_df)}\")\n",
    "\n",
    "# Filter: Keep rows where at least ONE nutrient is > 0\n",
    "valid_nutrition = (final_df['calories'] > 0) | (final_df['sugar'] > 0) | (final_df['fat'] > 0) | (final_df['protein'] > 0)\n",
    "clean_df = final_df[valid_nutrition].copy()\n",
    "\n",
    "print(f\"Cleaned Count: {len(clean_df)} (Removed {len(final_df) - len(clean_df)} empty rows)\")\n",
    "\n",
    "# 4. Save the Final Polish\n",
    "clean_output = os.path.join(PROJECT_DIR, \"india_products_clean.csv\")\n",
    "clean_df.to_csv(clean_output, index=False)\n",
    "\n",
    "print(f\"‚ú® DONE! Your Golden Database is at: {clean_output}\")\n",
    "display(clean_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ace24cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ RE-STARTING FINAL EXTRACTION (Universal Parser)...\n",
      "   Batch 1: Found 2 items.\n",
      "   Batch 2: Found 468 items.\n",
      "   Batch 3: Found 84 items.\n",
      "   Batch 4: Found 34 items.\n",
      "   Batch 5: Found 7 items.\n",
      "   Batch 6: Found 205 items.\n",
      "   Batch 7: Found 57 items.\n",
      "   Batch 8: Found 28 items.\n",
      "   Batch 9: Found 60 items.\n",
      "   Batch 10: Found 25 items.\n",
      "   Batch 11: Found 10 items.\n",
      "   Batch 12: Found 25 items.\n",
      "   Batch 13: Found 12 items.\n",
      "   Batch 14: Found 86 items.\n",
      "   Batch 15: Found 188 items.\n",
      "   Batch 16: Found 114 items.\n",
      "   Batch 17: Found 43 items.\n",
      "   Batch 18: Found 135 items.\n",
      "   Batch 19: Found 111 items.\n",
      "   Batch 20: Found 58 items.\n",
      "   Batch 21: Found 17 items.\n",
      "   Batch 22: Found 6 items.\n",
      "   Batch 23: Found 6 items.\n",
      "   Batch 24: Found 574 items.\n",
      "   Batch 25: Found 12 items.\n",
      "   Batch 26: Found 48 items.\n",
      "   Batch 27: Found 4 items.\n",
      "   Batch 28: Found 2 items.\n",
      "   Batch 29: Found 12 items.\n",
      "   Batch 31: Found 12 items.\n",
      "   Batch 32: Found 1 items.\n",
      "   Batch 33: Found 2 items.\n",
      "   Batch 34: Found 57 items.\n",
      "   Batch 35: Found 11 items.\n",
      "   Batch 36: Found 14 items.\n",
      "   Batch 37: Found 2 items.\n",
      "   Batch 38: Found 2 items.\n",
      "   Batch 39: Found 4 items.\n",
      "   Batch 40: Found 94 items.\n",
      "   Batch 42: Found 7 items.\n",
      "   Batch 43: Found 62 items.\n",
      "   Batch 44: Found 113 items.\n",
      "   Batch 45: Found 21 items.\n",
      "   Batch 46: Found 49 items.\n",
      "   Batch 47: Found 152 items.\n",
      "   Batch 48: Found 58 items.\n",
      "   Batch 49: Found 18 items.\n",
      "   Batch 50: Found 177 items.\n",
      "   Batch 51: Found 111 items.\n",
      "   Batch 52: Found 796 items.\n",
      "   Batch 53: Found 1220 items.\n",
      "   Batch 54: Found 411 items.\n",
      "   Batch 55: Found 21 items.\n",
      "   Batch 56: Found 97 items.\n",
      "   Batch 57: Found 28 items.\n",
      "   Batch 58: Found 87 items.\n",
      "   Batch 59: Found 232 items.\n",
      "   Batch 60: Found 143 items.\n",
      "   Batch 61: Found 99 items.\n",
      "   Batch 62: Found 86 items.\n",
      "   Batch 63: Found 63 items.\n",
      "   Batch 64: Found 82 items.\n",
      "   Batch 65: Found 55 items.\n",
      "   Batch 66: Found 69 items.\n",
      "   Batch 67: Found 67 items.\n",
      "   Batch 68: Found 75 items.\n",
      "   Batch 69: Found 66 items.\n",
      "   Batch 70: Found 49 items.\n",
      "   Batch 71: Found 65 items.\n",
      "   Batch 72: Found 70 items.\n",
      "   Batch 73: Found 54 items.\n",
      "   Batch 74: Found 67 items.\n",
      "   Batch 75: Found 59 items.\n",
      "   Batch 76: Found 62 items.\n",
      "   Batch 77: Found 101 items.\n",
      "   Batch 78: Found 82 items.\n",
      "   Batch 79: Found 76 items.\n",
      "   Batch 80: Found 90 items.\n",
      "   Batch 81: Found 81 items.\n",
      "   Batch 82: Found 85 items.\n",
      "   Batch 83: Found 84 items.\n",
      "   Batch 84: Found 61 items.\n",
      "   Batch 85: Found 91 items.\n",
      "   Batch 86: Found 99 items.\n",
      "   Batch 87: Found 104 items.\n",
      "   Batch 88: Found 94 items.\n",
      "   Batch 89: Found 85 items.\n",
      "   Batch 90: Found 71 items.\n",
      "   Batch 91: Found 76 items.\n",
      "   Batch 92: Found 69 items.\n",
      "   Batch 93: Found 54 items.\n",
      "   Batch 94: Found 62 items.\n",
      "   Batch 95: Found 56 items.\n",
      "   Batch 96: Found 69 items.\n",
      "   Batch 97: Found 54 items.\n",
      "   Batch 98: Found 69 items.\n",
      "   Batch 99: Found 70 items.\n",
      "   Batch 100: Found 75 items.\n",
      "   Batch 101: Found 56 items.\n",
      "   Batch 102: Found 63 items.\n",
      "   Batch 103: Found 60 items.\n",
      "   Batch 104: Found 47 items.\n",
      "   Batch 105: Found 55 items.\n",
      "   Batch 106: Found 54 items.\n",
      "   Batch 107: Found 56 items.\n",
      "   Batch 108: Found 49 items.\n",
      "   Batch 109: Found 65 items.\n",
      "   Batch 110: Found 38 items.\n",
      "   Batch 111: Found 54 items.\n",
      "   Batch 112: Found 62 items.\n",
      "   Batch 113: Found 51 items.\n",
      "   Batch 114: Found 69 items.\n",
      "   Batch 115: Found 70 items.\n",
      "   Batch 116: Found 73 items.\n",
      "   Batch 117: Found 83 items.\n",
      "   Batch 118: Found 79 items.\n",
      "   Batch 119: Found 3 items.\n",
      "   Batch 120: Found 11 items.\n",
      "   Batch 121: Found 16 items.\n",
      "   Batch 122: Found 27 items.\n",
      "   Batch 123: Found 14 items.\n",
      "   Batch 124: Found 4 items.\n",
      "   Batch 125: Found 10 items.\n",
      "   Batch 126: Found 20 items.\n",
      "   Batch 127: Found 6 items.\n",
      "   Batch 128: Found 10 items.\n",
      "   Batch 129: Found 34 items.\n",
      "   Batch 130: Found 9 items.\n",
      "   Batch 131: Found 47 items.\n",
      "   Batch 132: Found 58 items.\n",
      "   Batch 133: Found 77 items.\n",
      "   Batch 134: Found 67 items.\n",
      "   Batch 135: Found 68 items.\n",
      "   Batch 136: Found 69 items.\n",
      "   Batch 137: Found 65 items.\n",
      "   Batch 138: Found 49 items.\n",
      "   Batch 139: Found 53 items.\n",
      "   Batch 140: Found 57 items.\n",
      "   Batch 141: Found 58 items.\n",
      "   Batch 142: Found 64 items.\n",
      "   Batch 143: Found 59 items.\n",
      "   Batch 144: Found 62 items.\n",
      "   Batch 145: Found 70 items.\n",
      "   Batch 146: Found 58 items.\n",
      "   Batch 147: Found 65 items.\n",
      "   Batch 148: Found 59 items.\n",
      "   Batch 149: Found 52 items.\n",
      "   Batch 150: Found 58 items.\n",
      "   Batch 151: Found 43 items.\n",
      "   Batch 152: Found 59 items.\n",
      "   Batch 153: Found 50 items.\n",
      "   Batch 154: Found 38 items.\n",
      "   Batch 155: Found 41 items.\n",
      "   Batch 156: Found 34 items.\n",
      "   Batch 157: Found 45 items.\n",
      "   Batch 158: Found 42 items.\n",
      "   Batch 159: Found 47 items.\n",
      "   Batch 160: Found 53 items.\n",
      "   Batch 161: Found 40 items.\n",
      "   Batch 162: Found 38 items.\n",
      "   Batch 163: Found 61 items.\n",
      "   Batch 164: Found 43 items.\n",
      "   Batch 165: Found 57 items.\n",
      "   Batch 166: Found 50 items.\n",
      "   Batch 167: Found 46 items.\n",
      "   Batch 168: Found 54 items.\n",
      "   Batch 169: Found 50 items.\n",
      "   Batch 170: Found 62 items.\n",
      "   Batch 171: Found 69 items.\n",
      "   Batch 172: Found 59 items.\n",
      "   Batch 173: Found 74 items.\n",
      "   Batch 174: Found 60 items.\n",
      "   Batch 175: Found 66 items.\n",
      "   Batch 176: Found 56 items.\n",
      "   Batch 177: Found 72 items.\n",
      "   Batch 178: Found 64 items.\n",
      "   Batch 179: Found 52 items.\n",
      "   Batch 180: Found 77 items.\n",
      "   Batch 181: Found 40 items.\n",
      "   Batch 182: Found 43 items.\n",
      "   Batch 183: Found 51 items.\n",
      "   Batch 184: Found 65 items.\n",
      "   Batch 185: Found 57 items.\n",
      "   Batch 186: Found 52 items.\n",
      "   Batch 187: Found 52 items.\n",
      "   Batch 188: Found 52 items.\n",
      "   Batch 189: Found 28 items.\n",
      "   Batch 190: Found 49 items.\n",
      "   Batch 191: Found 60 items.\n",
      "   Batch 192: Found 64 items.\n",
      "   Batch 193: Found 50 items.\n",
      "   Batch 194: Found 49 items.\n",
      "   Batch 195: Found 47 items.\n",
      "   Batch 196: Found 34 items.\n",
      "   Batch 197: Found 38 items.\n",
      "   Batch 198: Found 45 items.\n",
      "   Batch 199: Found 45 items.\n",
      "   Batch 200: Found 59 items.\n",
      "\n",
      "üéâ DONE! Saved 14426 products to C:\\Users\\arjun\\Downloads\\Nutri Rate\\india_products_v2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barcode</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>grade</th>\n",
       "      <th>sugar</th>\n",
       "      <th>fat</th>\n",
       "      <th>protein</th>\n",
       "      <th>calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011433110587</td>\n",
       "      <td>[unknown]</td>\n",
       "      <td>deep indian kitchen</td>\n",
       "      <td>b</td>\n",
       "      <td>1.18</td>\n",
       "      <td>6.67</td>\n",
       "      <td>5.10</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011433119696</td>\n",
       "      <td>[pizza-average]</td>\n",
       "      <td>Deep Indian Kitchen</td>\n",
       "      <td>b</td>\n",
       "      <td>3.62</td>\n",
       "      <td>6.67</td>\n",
       "      <td>10.50</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0012000713019</td>\n",
       "      <td>[unknown]</td>\n",
       "      <td>Nestl√©</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0013454377024</td>\n",
       "      <td>[sauce-pesto-prepacked]</td>\n",
       "      <td>Stonemill Kitchens</td>\n",
       "      <td>e</td>\n",
       "      <td>0.00</td>\n",
       "      <td>166.00</td>\n",
       "      <td>25.50</td>\n",
       "      <td>1650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0013800143310</td>\n",
       "      <td>[unknown]</td>\n",
       "      <td>Nestl√©</td>\n",
       "      <td>c</td>\n",
       "      <td>3.72</td>\n",
       "      <td>3.72</td>\n",
       "      <td>6.05</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         barcode                     name                brand    grade  \\\n",
       "0  0011433110587                [unknown]  deep indian kitchen        b   \n",
       "1  0011433119696          [pizza-average]  Deep Indian Kitchen        b   \n",
       "2  0012000713019                [unknown]               Nestl√©  unknown   \n",
       "3  0013454377024  [sauce-pesto-prepacked]   Stonemill Kitchens        e   \n",
       "4  0013800143310                [unknown]               Nestl√©        c   \n",
       "\n",
       "   sugar     fat  protein  calories  \n",
       "0   1.18    6.67     5.10     122.0  \n",
       "1   3.62    6.67    10.50     267.0  \n",
       "2   0.00    0.00     0.00       0.0  \n",
       "3   0.00  166.00    25.50    1650.0  \n",
       "4   3.72    3.72     6.05     112.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "PROJECT_DIR = r\"C:\\Users\\arjun\\Downloads\\Nutri Rate\"\n",
    "PARQUET_FILE = r\"C:\\Users\\arjun\\Downloads\\food.parquet\"\n",
    "\n",
    "print(\"üöÄ RE-STARTING FINAL EXTRACTION (Universal Parser)...\")\n",
    "\n",
    "# --- 2. THE UNIVERSAL EXTRACTION FUNCTION ---\n",
    "def universal_extractor(blob, target_name):\n",
    "    \"\"\"\n",
    "    Handles List, String-of-List, or Struct.\n",
    "    \"\"\"\n",
    "    # CASE A: It's a String (The most likely culprit)\n",
    "    if isinstance(blob, str):\n",
    "        try:\n",
    "            # excessive safety: if it looks like a list, convert it\n",
    "            if blob.strip().startswith('['):\n",
    "                blob = ast.literal_eval(blob)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    # CASE B: It's a List (Standard)\n",
    "    if isinstance(blob, (list, np.ndarray)):\n",
    "        for item in blob:\n",
    "            if isinstance(item, dict) and item.get('name') == target_name:\n",
    "                val = item.get('100g')\n",
    "                if val is None: val = item.get('value')\n",
    "                \n",
    "                try: return float(val)\n",
    "                except: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "def clean_name(val):\n",
    "    \"\"\" Fixes the [{'text': 'Name'}] issue immediately \"\"\"\n",
    "    if isinstance(val, (list, np.ndarray)) and len(val) > 0:\n",
    "        if isinstance(val[0], dict): return val[0].get('text', 'Unknown')\n",
    "    return val\n",
    "\n",
    "# --- 3. BATCH PROCESS ---\n",
    "parquet_file = pq.ParquetFile(PARQUET_FILE)\n",
    "indian_products = []\n",
    "batch_count = 0\n",
    "\n",
    "for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "    batch_count += 1\n",
    "    df_chunk = batch.to_pandas()\n",
    "    \n",
    "    # A. Find Columns\n",
    "    # We prioritize 'nutriments' exactly\n",
    "    nutri_col = 'nutriments' if 'nutriments' in df_chunk.columns else None\n",
    "    \n",
    "    # Fallback search if exact name missing\n",
    "    if not nutri_col:\n",
    "        nutri_col = next((c for c in df_chunk.columns if 'nutriment' in c.lower()), None)\n",
    "\n",
    "    # B. Filter India\n",
    "    mask = pd.Series(False, index=df_chunk.index)\n",
    "    \n",
    "    # Check manufacturing places / country\n",
    "    place_col = next((c for c in df_chunk.columns if 'manufacturing' in c.lower() or 'country' in c.lower()), None)\n",
    "    if place_col:\n",
    "        mask |= df_chunk[place_col].astype(str).str.contains('india', case=False, na=False)\n",
    "        \n",
    "    # Check Brands\n",
    "    brand_col = next((c for c in df_chunk.columns if 'brand' in c.lower()), None)\n",
    "    if brand_col:\n",
    "        mask |= df_chunk[brand_col].astype(str).str.contains('Amul|Britannia|Parle|Tata|Haldiram|Nestle|Dabur|ITC', case=False, na=False)\n",
    "    \n",
    "    df_india = df_chunk[mask].copy()\n",
    "\n",
    "    # C. Process\n",
    "    if not df_india.empty:\n",
    "        # 1. Clean Names\n",
    "        name_col = next((c for c in df_india.columns if 'name' in c.lower() and 'generic' not in c.lower()), 'product_name')\n",
    "        if name_col in df_india.columns:\n",
    "             df_india['clean_name'] = df_india[name_col].apply(clean_name)\n",
    "        else:\n",
    "             df_india['clean_name'] = \"Unknown\"\n",
    "\n",
    "        # 2. Extract Nutrition\n",
    "        if nutri_col:\n",
    "            df_india['sugar'] = df_india[nutri_col].apply(lambda x: universal_extractor(x, 'sugars'))\n",
    "            df_india['fat'] = df_india[nutri_col].apply(lambda x: universal_extractor(x, 'fat')) # or 'saturated-fat'\n",
    "            df_india['protein'] = df_india[nutri_col].apply(lambda x: universal_extractor(x, 'proteins'))\n",
    "            df_india['calories'] = df_india[nutri_col].apply(lambda x: universal_extractor(x, 'energy-kcal'))\n",
    "        else:\n",
    "            # Emergency fill\n",
    "            df_india['sugar'] = 0.0; df_india['fat'] = 0.0; df_india['protein'] = 0.0; df_india['calories'] = 0.0\n",
    "\n",
    "        # Keep relevant columns\n",
    "        cols_to_keep = ['code', 'clean_name', 'brands', 'nutriscore_grade', 'image_url', 'sugar', 'fat', 'protein', 'calories']\n",
    "        available_cols = [c for c in cols_to_keep if c in df_india.columns]\n",
    "        \n",
    "        # Add the ones we just created manually\n",
    "        for c in ['clean_name', 'sugar', 'fat', 'protein', 'calories']:\n",
    "            if c not in available_cols: available_cols.append(c)\n",
    "\n",
    "        df_final_chunk = df_india[available_cols].copy()\n",
    "        indian_products.append(df_final_chunk)\n",
    "        \n",
    "        print(f\"   Batch {batch_count}: Found {len(df_final_chunk)} items.\")\n",
    "\n",
    "    # Stop at 200 batches for a good dataset (or remove limit)\n",
    "    if batch_count >= 200: break\n",
    "\n",
    "# --- 4. SAVE ---\n",
    "if indian_products:\n",
    "    final_df = pd.concat(indian_products, ignore_index=True)\n",
    "    \n",
    "    # Rename for final CSV\n",
    "    final_df = final_df.rename(columns={'code': 'barcode', 'clean_name': 'name', 'brands': 'brand', 'nutriscore_grade': 'grade'})\n",
    "    \n",
    "    # Filter out empty rows (Optional)\n",
    "    # final_df = final_df[ (final_df['calories'] > 0) | (final_df['sugar'] > 0) ]\n",
    "\n",
    "    output_path = os.path.join(PROJECT_DIR, \"india_products_v2.csv\")\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nüéâ DONE! Saved {len(final_df)} products to {output_path}\")\n",
    "    display(final_df.head())\n",
    "else:\n",
    "    print(\"‚ùå No items found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdbf0cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ PROCESSING COOKED INDIAN FOOD...\n",
      "Loaded 1014 raw dishes.\n",
      "Calculating Nutri-Scores for dishes...\n",
      "üéâ SUCCESS! Processed 1014 dishes.\n",
      "Saved to: C:\\Users\\arjun\\Downloads\\Nutri Rate\\indian_dishes_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barcode</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>grade</th>\n",
       "      <th>sugar</th>\n",
       "      <th>fat</th>\n",
       "      <th>protein</th>\n",
       "      <th>calories</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_barcode</td>\n",
       "      <td>Hot tea (Garam Chai)</td>\n",
       "      <td>Home/Restaurant</td>\n",
       "      <td>B</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IndianDishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_barcode</td>\n",
       "      <td>Instant coffee</td>\n",
       "      <td>Home/Restaurant</td>\n",
       "      <td>B</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IndianDishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_barcode</td>\n",
       "      <td>Espreso coffee</td>\n",
       "      <td>Home/Restaurant</td>\n",
       "      <td>B</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IndianDishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_barcode</td>\n",
       "      <td>Iced tea</td>\n",
       "      <td>Home/Restaurant</td>\n",
       "      <td>B</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IndianDishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no_barcode</td>\n",
       "      <td>Raw mango drink (Aam panna)</td>\n",
       "      <td>Home/Restaurant</td>\n",
       "      <td>B</td>\n",
       "      <td>9.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IndianDishes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      barcode                         name            brand grade  sugar  fat  \\\n",
       "0  no_barcode         Hot tea (Garam Chai)  Home/Restaurant     B   2.58  0.0   \n",
       "1  no_barcode               Instant coffee  Home/Restaurant     B   3.65  0.0   \n",
       "2  no_barcode               Espreso coffee  Home/Restaurant     B   6.62  0.0   \n",
       "3  no_barcode                     Iced tea  Home/Restaurant     B   2.70  0.0   \n",
       "4  no_barcode  Raw mango drink (Aam panna)  Home/Restaurant     B   9.05  0.0   \n",
       "\n",
       "   protein  calories        source  \n",
       "0     0.39       0.0  IndianDishes  \n",
       "1     0.64       0.0  IndianDishes  \n",
       "2     1.75       0.0  IndianDishes  \n",
       "3     0.03       0.0  IndianDishes  \n",
       "4     0.16       0.0  IndianDishes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "PROJECT_DIR = r\"C:\\Users\\arjun\\Downloads\\Nutri Rate\"\n",
    "# Make sure this matches the exact name of your downloaded file\n",
    "INDIAN_CSV = r\"C:\\Users\\arjun\\Downloads\\Indian_Food_Nutrition_Processed.csv\"\n",
    "\n",
    "print(\"üöÄ PROCESSING COOKED INDIAN FOOD...\")\n",
    "\n",
    "# --- 2. DEFINE GRADING ALGORITHM (The \"AI\" Logic) ---\n",
    "def calculate_grade(row):\n",
    "    \"\"\"\n",
    "    Calculates a simple A-E grade based on Indian Diet context.\n",
    "    \"\"\"\n",
    "    # 1. Negative Points (Bad stuff)\n",
    "    points = 0\n",
    "    \n",
    "    # Sugar (High penalty)\n",
    "    if row['sugar'] > 22.5: points += 10\n",
    "    elif row['sugar'] > 10: points += 6\n",
    "    elif row['sugar'] > 4.5: points += 2\n",
    "    \n",
    "    # Fat (Medium penalty)\n",
    "    if row['fat'] > 15: points += 5\n",
    "    elif row['fat'] > 5: points += 2\n",
    "    \n",
    "    # Calories (Density penalty)\n",
    "    if row['calories'] > 300: points += 4\n",
    "    elif row['calories'] > 150: points += 2\n",
    "    \n",
    "    # 2. Positive Points (Good stuff)\n",
    "    # Protein (Bonus)\n",
    "    if row['protein'] > 15: points -= 5\n",
    "    elif row['protein'] > 8: points -= 3\n",
    "    \n",
    "    # 3. Final Score Mapping\n",
    "    if points <= -2: return 'A'\n",
    "    if points <= 2: return 'B'\n",
    "    if points <= 8: return 'C'\n",
    "    if points <= 15: return 'D'\n",
    "    return 'E'\n",
    "\n",
    "# --- 3. LOAD & CLEAN ---\n",
    "try:\n",
    "    df_dishes = pd.read_csv(INDIAN_CSV)\n",
    "    print(f\"Loaded {len(df_dishes)} raw dishes.\")\n",
    "    \n",
    "    # Inspect columns to map them correctly\n",
    "    # Usually these datasets have 'Carbohydrates', 'Total Fat', 'Protein', 'Energy'\n",
    "    # We rename them to our standard schema\n",
    "    \n",
    "    # NOTE: Adjust these keys if your CSV has different names (e.g. \"Carbs\" vs \"Carbohydrates\")\n",
    "    # This mapping is based on the Kaggle dataset you described earlier\n",
    "    rename_map = {\n",
    "        'Dish Name': 'name',\n",
    "        'Calories': 'calories', \n",
    "        'Protein (g)': 'protein',\n",
    "        'Fat (g)': 'fat',\n",
    "        'Carbohydrates (g)': 'sugar', # Using Carbs as Sugar proxy for cooked food\n",
    "    }\n",
    "    \n",
    "    # If columns are missing, try generic names\n",
    "    if 'Dish Name' not in df_dishes.columns:\n",
    "        # Fallback for standard Kaggle \"indian_food.csv\"\n",
    "        rename_map = {\n",
    "            'name': 'name',\n",
    "            'calories': 'calories',\n",
    "            'protein': 'protein', \n",
    "            'fats': 'fat',\n",
    "            'carbohydrates': 'sugar'\n",
    "        }\n",
    "\n",
    "    df_dishes = df_dishes.rename(columns=rename_map)\n",
    "    \n",
    "    # Ensure all numeric columns exist and are numbers\n",
    "    for col in ['calories', 'protein', 'fat', 'sugar']:\n",
    "        if col not in df_dishes.columns:\n",
    "            df_dishes[col] = 0.0\n",
    "        else:\n",
    "            # Force convert to number, coerce errors to 0\n",
    "            df_dishes[col] = pd.to_numeric(df_dishes[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # --- 4. CALCULATE GRADES ---\n",
    "    print(\"Calculating Nutri-Scores for dishes...\")\n",
    "    df_dishes['grade'] = df_dishes.apply(calculate_grade, axis=1)\n",
    "    \n",
    "    # Add metadata\n",
    "    df_dishes['brand'] = 'Home/Restaurant'\n",
    "    df_dishes['barcode'] = 'no_barcode' # Cooked food has no barcode\n",
    "    df_dishes['source'] = 'IndianDishes'\n",
    "    \n",
    "    # Keep only standard columns\n",
    "    final_cols = ['barcode', 'name', 'brand', 'grade', 'sugar', 'fat', 'protein', 'calories', 'source']\n",
    "    df_dishes_clean = df_dishes[final_cols].copy()\n",
    "    \n",
    "    # --- 5. SAVE ---\n",
    "    output_path = os.path.join(PROJECT_DIR, \"indian_dishes_clean.csv\")\n",
    "    df_dishes_clean.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"üéâ SUCCESS! Processed {len(df_dishes_clean)} dishes.\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    display(df_dishes_clean.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Columns found in file:\", df_dishes.columns.tolist() if 'df_dishes' in locals() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6008e719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ MERGING EVERYTHING...\n",
      "üèÜ CHAMPION! Master Database Created with 15440 items.\n",
      "üìÇ Location: C:\\Users\\arjun\\Downloads\\Nutri Rate\\master_food_database.csv\n",
      "\n",
      "Sample Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barcode</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>grade</th>\n",
       "      <th>sugar</th>\n",
       "      <th>fat</th>\n",
       "      <th>protein</th>\n",
       "      <th>calories</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9785</th>\n",
       "      <td>42272905355</td>\n",
       "      <td>['soup-average']</td>\n",
       "      <td>Amy's Kitchen Inc.</td>\n",
       "      <td>b</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.76</td>\n",
       "      <td>67.800003</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15258</th>\n",
       "      <td>no_barcode</td>\n",
       "      <td>Lotus stem pickle (Kamal kakdi ka achar)</td>\n",
       "      <td>Home/Restaurant</td>\n",
       "      <td>C</td>\n",
       "      <td>20.070000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>IndianDishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>3023290220689</td>\n",
       "      <td>['unknown']</td>\n",
       "      <td>Nestl√©</td>\n",
       "      <td>d</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>5.90</td>\n",
       "      <td>4.70</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>7613035357549</td>\n",
       "      <td>['unknown']</td>\n",
       "      <td>Nestl√©</td>\n",
       "      <td>unknown</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>31.10</td>\n",
       "      <td>6.30</td>\n",
       "      <td>542.000000</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>4600680014982</td>\n",
       "      <td>Unknown Product</td>\n",
       "      <td>Nestle</td>\n",
       "      <td>e</td>\n",
       "      <td>48.900002</td>\n",
       "      <td>27.60</td>\n",
       "      <td>5.70</td>\n",
       "      <td>521.000000</td>\n",
       "      <td>OpenFoodFacts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             barcode                                      name  \\\n",
       "9785     42272905355                          ['soup-average']   \n",
       "15258     no_barcode  Lotus stem pickle (Kamal kakdi ka achar)   \n",
       "2033   3023290220689                               ['unknown']   \n",
       "9281   7613035357549                               ['unknown']   \n",
       "12595  4600680014982                           Unknown Product   \n",
       "\n",
       "                    brand    grade      sugar    fat  protein    calories  \\\n",
       "9785   Amy's Kitchen Inc.        b   2.010000   1.76     2.76   67.800003   \n",
       "15258     Home/Restaurant        C  20.070000   0.00     1.44    0.000000   \n",
       "2033               Nestl√©        d  19.500000   5.90     4.70  158.000000   \n",
       "9281               Nestl√©  unknown  57.500000  31.10     6.30  542.000000   \n",
       "12595              Nestle        e  48.900002  27.60     5.70  521.000000   \n",
       "\n",
       "              source  \n",
       "9785   OpenFoodFacts  \n",
       "15258   IndianDishes  \n",
       "2033   OpenFoodFacts  \n",
       "9281   OpenFoodFacts  \n",
       "12595  OpenFoodFacts  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"üîÑ MERGING EVERYTHING...\")\n",
    "\n",
    "# 1. Load Packaged Food\n",
    "packaged_file = os.path.join(PROJECT_DIR, \"india_products_v2.csv\")\n",
    "df_packaged = pd.read_csv(packaged_file)\n",
    "df_packaged['source'] = 'OpenFoodFacts'\n",
    "\n",
    "# 2. Load Cooked Food\n",
    "cooked_file = os.path.join(PROJECT_DIR, \"indian_dishes_clean.csv\")\n",
    "if os.path.exists(cooked_file):\n",
    "    df_cooked = pd.read_csv(cooked_file)\n",
    "else:\n",
    "    df_cooked = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è Warning: Cooked food file not found. Skipping.\")\n",
    "\n",
    "# 3. Combine\n",
    "master_df = pd.concat([df_packaged, df_cooked], ignore_index=True)\n",
    "\n",
    "# 4. Final Polish (Fill NaNs with 0 or Unknown)\n",
    "master_df['name'] = master_df['name'].fillna(\"Unknown Product\")\n",
    "master_df['brand'] = master_df['brand'].fillna(\"Unknown Brand\")\n",
    "master_df[['sugar', 'fat', 'protein', 'calories']] = master_df[['sugar', 'fat', 'protein', 'calories']].fillna(0.0)\n",
    "\n",
    "# 5. Save the API Database\n",
    "master_path = os.path.join(PROJECT_DIR, \"master_food_database.csv\")\n",
    "master_df.to_csv(master_path, index=False)\n",
    "\n",
    "print(f\"üèÜ CHAMPION! Master Database Created with {len(master_df)} items.\")\n",
    "print(f\"üìÇ Location: {master_path}\")\n",
    "print(\"\\nSample Data:\")\n",
    "display(master_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1ebf751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç INSPECTING BARCODE: 978542272905355\n",
      "\n",
      "‚ùå CRITICAL: The barcode 978542... is NOT in the database at all.\n",
      "Let's check the first 5 barcodes in the file to see the format:\n",
      "['11433110587', '11433119696', '12000713019', '13454377024', '13800143310']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# CONFIG\n",
    "CSV_PATH = r\"C:\\Users\\arjun\\Downloads\\Nutri Rate\\master_food_database.csv\"\n",
    "\n",
    "print(\"üîç INSPECTING BARCODE: 978542272905355\")\n",
    "\n",
    "# 1. Load the dataframe WITHOUT any type conversion first\n",
    "df = pd.read_csv(CSV_PATH, dtype=object) \n",
    "\n",
    "# 2. Search for the row using loose matching\n",
    "# We look for any row where the barcode *contains* \"978542\" (first 6 digits)\n",
    "# This helps us find it even if the end is cut off or formatted weirdly.\n",
    "mask = df['barcode'].astype(str).str.contains(\"978542\", na=False)\n",
    "results = df[mask]\n",
    "\n",
    "if not results.empty:\n",
    "    print(f\"\\n‚úÖ Found {len(results)} potential matches.\")\n",
    "    print(\"Here is exactly how they look in the CSV:\")\n",
    "    \n",
    "    for idx, row in results.iterrows():\n",
    "        raw_val = row['barcode']\n",
    "        name = row['name']\n",
    "        print(f\"\\n--- Row {idx} ---\")\n",
    "        print(f\"Product: {name}\")\n",
    "        print(f\"RAW BARCODE VALUE: '{raw_val}'\")\n",
    "        print(f\"Type: {type(raw_val)}\")\n",
    "        \n",
    "        # Test if it equals the target\n",
    "        target = \"978542272905355\"\n",
    "        print(f\"Match Check: '{raw_val}' == '{target}' ? {str(raw_val) == target}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå CRITICAL: The barcode 978542... is NOT in the database at all.\")\n",
    "    print(\"Let's check the first 5 barcodes in the file to see the format:\")\n",
    "    print(df['barcode'].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0a422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nutrirate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
